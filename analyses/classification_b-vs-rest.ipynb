{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet_ml import Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn 0.22 needed for permutation importance\n",
    "import sys\n",
    "sys.path.insert(0, \"/eos/user/s/sbysiak/.local/lib/python3.7/site-packages/\")\n",
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "\n",
    "import uproot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score as acc, f1_score, roc_curve, roc_auc_score, classification_report, confusion_matrix, auc\n",
    "from sklearn.inspection import permutation_importance\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper.plotting import plot_roc, plot_score_vs_pt, plot_score_vs_col, plot_tagging_eff, plot_confusion_matrix, plot_xgb_learning_curve, plot_score_distr, plot_signal_significance, plot_eff_vs_threshold, plot_pdp\n",
    "from helper.utils import signal_eff, get_optimal_threshold, convert_float64_to_float32, save_model, printmd\n",
    "from helper.interpret import feature_importance_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.size']=16\n",
    "pd.options.display.max_columns = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [c for c in df.columns if 'Track_0_' in c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    del df\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_weight(target_pt, real_pt):\n",
    "    weight_dict = {}\n",
    "    pt_bins = np.arange(target_pt.min(), target_pt.max()+5, 5)\n",
    "    for low,high in zip(pt_bins[:-1], pt_bins[1:]):\n",
    "        n_target = np.sum( (target_pt > low) & (target_pt < high))\n",
    "        n_real = np.sum((real_pt > low) & (real_pt < high))\n",
    "        for pt in range(int(low), int(high)):\n",
    "            weight_dict[pt] = n_target / n_real\n",
    "    return [weight_dict[int(pt)] for pt in real_pt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows_b    = 200000\n",
    "nrows_c    = 100000\n",
    "nrows_udsg = 100000\n",
    "custom_descr = 'Tr-sortbyIPdNsigmaAbs-noCuts_SV-sortbyLxyNsigma-noCuts'\n",
    "# custom_descr = 'Tr-sortbyPt-cuts-IPdLT02_SV-sortbyDispersion-noCuts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_b = pd.read_csv(f'datasets/iter2/bjets_10-150GeV_{custom_descr}.csv', nrows=nrows_b)\n",
    "# df_b = df_b.sample(n=int(nrows_b/2), weights=df_b['Jet_Pt'].apply(lambda pt: 1 if pt < 50 else 0.1))\n",
    "df_b['flavour'] = 'b'\n",
    "df_b = convert_float64_to_float32(df_b)\n",
    "# df_b.describe()\n",
    "# df_b['Jet_Pt'].describe([0.01, 0.1, 0.25, 0.5, 0.75, 0.9, 0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c = pd.read_csv(f'datasets/iter2/cjets_10-150GeV_{custom_descr}.csv', nrows=nrows_c*5)\n",
    "df_c['flavour'] = 'c'\n",
    "df_c = convert_float64_to_float32(df_c)\n",
    "\n",
    "weights = assign_weight(df_b['Jet_Pt'], df_c['Jet_Pt'])\n",
    "df_c = df_c.sample(weights=weights, n=nrows_c, replace=False) \n",
    "\n",
    "ax=df_b['Jet_Pt'].hist(bins=100, alpha=0.5, density=1)\n",
    "df_c['Jet_Pt'].hist(bins=100, alpha=0.5, density=1, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_udsg = pd.read_csv(f'datasets/iter2/udsgjets_10-150GeV_{custom_descr}.csv', nrows=nrows_udsg*5)\n",
    "df_udsg['flavour'] = 'udsg'\n",
    "df_udsg = convert_float64_to_float32(df_udsg)\n",
    "# df_udsg.describe()\n",
    "\n",
    "weights = assign_weight(df_b['Jet_Pt'], df_udsg['Jet_Pt'])\n",
    "df_udsg = df_udsg.sample(weights=weights, n=nrows_udsg, replace=False) \n",
    "\n",
    "ax=df_b['Jet_Pt'].hist(bins=100, alpha=0.5, density=1)\n",
    "df_udsg['Jet_Pt'].hist(bins=100, alpha=0.5, density=1, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_b, df_c, df_udsg])\n",
    "n_b_jets, n_c_jets, n_udsg_jets = len(df_b), len(df_c), len(df_udsg)\n",
    "del df_b\n",
    "del df_c\n",
    "del df_udsg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preserve pT in case it will not be used for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptbins = np.array([df['Jet_Pt'].min()-1e-6] + [20, 30, 40, 50, 60, 70, 80, 90, 100] + [df['Jet_Pt'].max()+1e-6])\n",
    "flavour_ptbin = df[['flavour', 'Jet_Pt']].apply(lambda row: (row['flavour']+str(sum(row['Jet_Pt'] >= np.array(ptbins)))), axis=1)\n",
    "pt_bin_arr = df['Jet_Pt'].apply(lambda pt: str(sum(pt >= ptbins)))\n",
    "flavour_ptbin = df['flavour'] + pt_bin_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__\\>\\> Select columns HERE (before logging data info) <<__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[c for c in df.columns if 'Track_0_' in c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[[col for col in df.columns if 'Jet_SecVtx_' not in col]]\n",
    "n_tracks, n_sv = 10,3\n",
    "filter_tracks = lambda col: ('Jet_Track'  in col and int(col.split('_')[2]) < n_tracks \n",
    "                             and 'PID' not in col \n",
    "#                              and '_Pt_' not in col\n",
    "#                              and col.split('_')[3] in ['Pt', 'Phi', 'Eta', 'IPdSigma', 'IPzSigma'] + ['IPdNsigmaAbs', 'IPzNsigmaAbs']\n",
    "                            )\n",
    "filter_sv     = lambda col: ('Jet_SecVtx' in col and int(col.split('_')[2]) < n_sv)\n",
    "filter_jet    = lambda col: ('Jet_Track'  not in col and 'Jet_SecVtx' not in col \n",
    "#                              and col != 'Jet_Pt'\n",
    "                            )\n",
    "filter_cols   = lambda col: ((filter_tracks(col) or filter_sv(col) or filter_jet(col) or col == 'flavour')\n",
    "                            and 'DerivCorr' not in col\n",
    "#                             and 'Nsigma__sortby' in col or 'flavour' in col or 'ptbin' in col\n",
    "                            )\n",
    "df = df[[col for col in df.columns if filter_cols(col)]]\n",
    "\n",
    "# remove features correlated with pt\n",
    "# corr = df.corr()\n",
    "# corr_thresh = 0.20\n",
    "# pt_correlated = corr['Jet_Pt'][abs(corr['Jet_Pt']) > corr_thresh]\n",
    "# df = df[[col for col in df.columns if col not in pt_correlated.index.to_list()]]\n",
    "# print(f'removed pt-correlated features (threshold = {corr_thresh:.2f}):\\n{pt_correlated}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info(verbose=False, memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset (DataFrame -> X & y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data using `stratify` with `flavour` and `Jet_Pt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['flavour'].map({'b':1, 'c':0, 'udsg':0})\n",
    "X = df.drop(['flavour', 'ptbin'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, flavour_ptbin_train, flavour_ptbin_test = train_test_split(X, y, flavour_ptbin, test_size=0.2, stratify=flavour_ptbin, random_state=122)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pt_spectra(pt_train, pt_test, bins=np.linspace(10,150,29), color='k', label='', ax=None):\n",
    "    density = 0\n",
    "    if not ax: \n",
    "        fig,ax = plt.subplots(figsize=(10,7))\n",
    "    ax.hist(pt_test    , bins=bins, histtype='step', lw=2, density=density, label='test '+label     , linestyle='-', color=color);\n",
    "    ax.hist(pt_train   , bins=bins, histtype='step', lw=2, density=density, label='train '+label    , linestyle='--', color=color);\n",
    "    ax.semilogy()\n",
    "    ax.legend();\n",
    "    ax.grid(linestyle=':')\n",
    "    ax.set_xlabel('jet $p_T^{reco}$ [GeV/c]')\n",
    "    ax.set_ylabel('counts')\n",
    "    return ax\n",
    "    \n",
    "# b & c together    \n",
    "if 'Jet_Pt' in X_train.columns:\n",
    "    ax = plot_pt_spectra(X_train['Jet_Pt'][y_train==1], X_test['Jet_Pt'][y_test==1], label='b', color='r')\n",
    "    ax = plot_pt_spectra(X_train['Jet_Pt'][y_train==0], X_test['Jet_Pt'][y_test==0], label='c+udsg', color='b', ax=ax)\n",
    "\n",
    "# b & c separately\n",
    "# ax = plot_pt_spectra(X_train['Jet_Pt'][np.array(['b' in fp for fp in  flavour_ptbin_train])], X_test['Jet_Pt'][np.array(['b' in fp for fp in  flavour_ptbin_test])], label='b', color='r')\n",
    "# ax = plot_pt_spectra(X_train['Jet_Pt'][np.array(['c' in fp for fp in  flavour_ptbin_train])], X_test['Jet_Pt'][np.array(['c' in fp for fp in  flavour_ptbin_test])], label='c', color='orange', ax=ax)\n",
    "# ax = plot_pt_spectra(X_train['Jet_Pt'][np.array(['udsg' in fp for fp in  flavour_ptbin_train])], X_test['Jet_Pt'][np.array(['udsg' in fp for fp in  flavour_ptbin_test])], label='udsg', color='b', ax=ax)\n",
    "\n",
    "# plt.savefig('pt_spect.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test  = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create experiment and log data info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    exp.end()\n",
    "except:\n",
    "    pass\n",
    "exp = Experiment(\n",
    "                 auto_output_logging='simple',\n",
    "                 log_env_gpu=False, log_env_cpu=False,\n",
    "                 project_name=\"test\", workspace=\"phd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.add_tags(['b-vs-rest'])\n",
    "if nrows_c == nrows_udsg: exp.add_tag('N_c = N_udsg')\n",
    "else: exp.add_tag('N_c =/= N_udsg')\n",
    "\n",
    "n_jets_str = f'there is:\\n\\t{n_b_jets} b jets\\n\\t{n_c_jets} c jets\\n\\t{n_udsg_jets} udsg jets'\n",
    "dataset_info = n_jets_str + f'\\ndataframe size = {df.memory_usage(deep=True).sum()/1024/1024:.1f} MB'\n",
    "print(dataset_info)\n",
    "print(df.columns.to_list())\n",
    "exp.log_dataset_info(dataset_info)\n",
    "exp.log_dataset_hash(df)\n",
    "exp.log_other('n_jets_b', n_b_jets)\n",
    "exp.log_other('n_jets_c', n_c_jets)\n",
    "exp.log_other('n_jets_udsg', n_udsg_jets)\n",
    "exp.log_other('n_jets_rest', n_c_jets+n_udsg_jets)\n",
    "exp.log_other('n_columns', X.shape[1])\n",
    "\n",
    "exp.log_parameter('n_tracks', n_tracks)\n",
    "exp.log_parameter('n_sv', n_sv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.add_tag('full-info')\n",
    "exp.log_other('descr', f'{custom_descr}, adjusted pT, n_tr={n_tracks}, n_sv={n_sv}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_iter = 0\n",
    "\n",
    "def xgb_callback(y_pred, dtrain, mistag_rates=[0.1, 0.01, 0.001], make_plots=False):\n",
    "    global training_iter\n",
    "    y_true = dtrain.get_label()\n",
    "    metrics = []\n",
    "    for mistag_rate in mistag_rates:\n",
    "        metrics.append((f'bEff@mistag_{mistag_rate:.0e}', signal_eff(y_true, y_pred, mistag_rate)))\n",
    "    metrics.append(('ROC_AUC', roc_auc_score(y_true, y_pred)))\n",
    "    if any([' ' in met_name or ':' in met_name for met_name, _ in metrics]):\n",
    "        raise ValueError('Metric names cannot contain space nor colon(:)')\n",
    "\n",
    "    if not make_plots: \n",
    "        return metrics\n",
    "    is_testset = False\n",
    "    if len(y_true) == len(y_test):\n",
    "        is_testset = all(y_true == y_test)\n",
    "    if (not (training_iter % 30)) or training_iter in [0,1,3]:\n",
    "        if not is_testset:\n",
    "            ax = plot_tagging_eff(y_true, y_pred, label='train', color='r' if is_testset else 'b')\n",
    "        else:\n",
    "            ax = plot_tagging_eff(y_true, y_pred, label='test', color='r' if is_testset else 'b', ax=plt.gca())\n",
    "            ax.set_ylim(1e-4, 2)\n",
    "            exp.log_figure(f'plot_iter{training_iter:04}')        \n",
    "    if is_testset:\n",
    "        training_iter += 1        \n",
    "    return metrics\n",
    "\n",
    "\n",
    "params = dict(n_estimators=100, learning_rate=0.2, \n",
    "              max_depth=5, tree_method='hist', \n",
    "              gamma=10, reg_lambda=0,\n",
    "              subsample=0.8, colsample_bytree=0.8, colsample_bynode=0.8,\n",
    "              scale_pos_weight=(sum(y==0)/sum(y==1)), random_state=123,\n",
    "             )\n",
    "    \n",
    "exp.add_tag('XGB')\n",
    "exp.log_parameters(params, prefix='manual')  # backward compatibility\n",
    "exp.log_parameters(params, prefix='man')\n",
    "clf = XGBClassifier(**params)\n",
    "clf.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_test, y_test)], eval_metric=partial(xgb_callback, make_plots=True), verbose=10)\n",
    "# exp.send_notification(title='COMETML - test done', status='training finished', additional_data='No need of additional data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_res = clf.evals_result()\n",
    "for metric in eval_res['validation_0'].keys():\n",
    "    print(metric)\n",
    "    ax = plot_xgb_learning_curve(eval_res, metric)\n",
    "    exp.log_figure(f'{metric}_vs_ntrees')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(clf, X.columns, scaler, exp, 'xgb_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_proba = clf.predict_proba(X_train)[:,1]\n",
    "y_test_proba = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "y_train_pred = clf.predict(X_train)\n",
    "y_test_pred = clf.predict(X_test)\n",
    "\n",
    "opt_thresh = get_optimal_threshold(y_train, y_train_proba, 0.04)\n",
    "y_train_pred_opt = (y_train_proba > opt_thresh).astype('int')\n",
    "y_test_pred_opt  = (y_test_proba  > opt_thresh).astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scores distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(nrows=2, figsize=(12,7), gridspec_kw={'height_ratios': [2,1]})\n",
    "plot_score_distr(y_train, y_train_proba, linestyle=':', ax=axes[0])\n",
    "plot_score_distr(y_test , y_test_proba , linestyle='-', ax=axes[0], lw=2)\n",
    "plot_signal_significance(y_train, y_train_proba, 0.02,    linestyle=':', color='cyan'   ,  label='b frac. = 2%', ax=axes[1])\n",
    "plot_signal_significance(y_train, y_train_proba, 0.04,    linestyle=':', color='lime'   ,  label='b frac. = 4%', ax=axes[1])\n",
    "plot_signal_significance(y_train, y_train_proba, 0.08,    linestyle=':', color='magenta',  label='b frac. = 8%', ax=axes[1])\n",
    "# plot_signal_significance(y_test , y_test_proba , 0.01,    linestyle='-', color='lime'   , lw=2, label='b frac. = 1%', ax=axes[1])\n",
    "# plot_signal_significance(y_test , y_test_proba , 0.04,    linestyle='-', color='magenta', lw=2, label='b frac. = 4%', ax=axes[1])\n",
    "axes[0].vlines(opt_thresh, *axes[0].get_ylim(), color='lime', lw=2, linestyle=':')\n",
    "exp.log_figure('score_and_significance_vs_threshold')\n",
    "\n",
    "xmax = max(max(y_train_proba), max(y_test_proba))\n",
    "axes[0].set_xlim(xmax-0.2, xmax+0.01)\n",
    "axes[1].set_xlim(xmax-0.2, xmax+0.01)\n",
    "axes[1].set_ylim(0.95,1)\n",
    "exp.log_figure('score_and_significance_vs_threshold_zoom')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eff vs threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_eff_vs_threshold(y_test, y_test_proba)\n",
    "exp.log_figure('eff_vs_threshold')\n",
    "ax.set_xlim(0.6,1)\n",
    "exp.log_figure('eff_vs_threshold_zoom')\n",
    "ax.set_yscale('log')\n",
    "exp.log_figure('eff_vs_threshold_logy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC - log AUC scores and plot vs pT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_roc(y_train, y_train_proba, label='train', color='b');\n",
    "ax = plot_roc(y_test, y_test_proba, label='test' , color='r', ax=ax);\n",
    "exp.log_figure('roc_curve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.log_metric('roc_auc_test', roc_auc_score(y_test, y_test_proba))\n",
    "exp.log_metric('roc_auc_train', roc_auc_score(y_train, y_train_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_score_vs_pt(y_train, y_train_pred, y_train_proba, flavour_ptbin_train, ptbins, score=(roc_auc_score, 'ROC AUC'), label='train', marker='o', color='b')\n",
    "ax = plot_score_vs_pt(y_test, y_test_pred, y_test_proba, flavour_ptbin_test , ptbins, score=(roc_auc_score, 'ROC AUC'), label='test' , marker='^', color='r', ax=ax)\n",
    "exp.log_figure('roc_auc_vs_pt');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aver_pred(y_true, y_score):\n",
    "    return np.average(y_score)\n",
    "\n",
    "ax = plot_score_vs_pt(y_train, y_train_pred, y_train_proba, flavour_ptbin_train, ptbins, score=(aver_pred, 'aver. pred'), label='train', marker='o', color='b')\n",
    "ax = plot_score_vs_pt(y_test, y_test_pred, y_test_proba, flavour_ptbin_test , ptbins, score=(aver_pred, 'aver. score'), label='test' , marker='^', color='r', ax=ax)\n",
    "exp.log_figure('aver_score_vs_pt');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mistagging rate VS tagging efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_tagging_eff(y_test, y_test_proba, label='$b$ vs $c+udsg$ test', color='r')\n",
    "plot_tagging_eff(y_train, y_train_proba, label='$b$ vs $c+udsg$ train', color='b', ax=ax)\n",
    "exp.log_figure('tagging_eff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistag_rates = [0.1, 0.01, 0.001]\n",
    "for mistag_rate in mistag_rates:\n",
    "    eff = signal_eff(y_test, y_test_proba, mistag_rate)\n",
    "    exp.log_metric(f'tagEff@mistag_{mistag_rate:.0e}', eff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printmd('__TRAIN__')\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10,5))\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(wspace=0.5)\n",
    "plot_confusion_matrix(y_train, y_train_pred_opt, ['c+udsg', 'b'], title='train, unnormalized', normalize=False, ax=axes[0])\n",
    "plot_confusion_matrix(y_train, y_train_pred_opt, ['c+udsg', 'b'], title='train, normalized'  , normalize=True , ax=axes[1])\n",
    "exp.log_figure('confusion_matrix_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printmd('__TEST__')\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10,5))\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(wspace=0.5)\n",
    "plot_confusion_matrix(y_test, y_test_pred_opt, ['c+udsg', 'b'], title='test, unnormalized', normalize=False, ax=axes[0])\n",
    "plot_confusion_matrix(y_test, y_test_pred_opt, ['c+udsg', 'b'], title='test, normalized'  , normalize=True , ax=axes[1])\n",
    "exp.log_figure('confusion_matrix_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance vs _feature_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import helper\n",
    "reload(helper)\n",
    "reload(helper.plotting)\n",
    "reload(helper.plotting.performance_plots)\n",
    "plot_score_vs_col = helper.plotting.plot_score_vs_col\n",
    "plot_pdp = helper.plotting.plot_pdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aver_pred(y_true, y_score):\n",
    "    return np.average(y_score)\n",
    "\n",
    "\n",
    "for feature,bins,bins_distplot in [\n",
    "                     ('Jet_Pt', (10,20,30,40,50,60,80,100), 100), \n",
    "                     ('Jet_Phi', 18*3, 10), \n",
    "                     ('Jet_Eta', 20, 100), \n",
    "                     ('Jet_NumTracks', np.arange(0,30,2), np.arange(0,30,1)),\n",
    "                      ]:\n",
    "    if feature not in X.columns: continue\n",
    "    for score in [(roc_auc_score, 'ROC AUC'), \n",
    "                  (partial(signal_eff, mistag_rate_thresh=1e-2), 'signal eff for mistag=1e-2'),\n",
    "                  (aver_pred, 'aver. score')\n",
    "                    ]:\n",
    "        ax=plot_score_vs_col(y_train, y_train_proba, \n",
    "                      vals=scaler.inverse_transform(X_train)[:, df.columns.get_loc(feature) ], \n",
    "                      bins=bins, bins_distplot=bins_distplot,\n",
    "                      score=score, color='b', label='train',\n",
    "                      show_distplot=True,                          \n",
    "                      show_errorbars=True,\n",
    "                     )\n",
    "        plot_score_vs_col(y_test, y_test_proba, \n",
    "                          vals=scaler.inverse_transform(X_test)[:, df.columns.get_loc(feature) ],\n",
    "                          bins=bins, bins_distplot=bins_distplot,\n",
    "                          score=score, color='r', marker='^', label='test', \n",
    "                          xlabel=feature,\n",
    "                          show_distplot=True,\n",
    "                          show_errorbars=True,\n",
    "                          ax=ax\n",
    "                         )\n",
    "        exp.log_figure(f\"{score[1].replace(' ', '').replace('=','-').replace('.', '-')}_vs_{feature.replace('Jet', '')}\");\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGB's weight**   \n",
    "  = how many times feature was used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_report(clf.feature_importances_, X.columns, importance_type='XGB\\'s weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGB's total gain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_dict = clf.get_booster().get_score(importance_type='total_gain')\n",
    "imp = imp_dict.values()\n",
    "names = X.columns[[int(k[1:]) for k in imp_dict.keys()]]\n",
    "feature_importance_report(imp, names, importance_type='XGB\\'s total_gain')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Permutation importance**  \n",
    "remember to use scaled input data  \n",
    "it also quite time-consuming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = permutation_importance(clf,X_train[:10000],y_train[:10000])['importances_mean']\n",
    "feature_importance_report(imp, X.columns, importance_type='permutation imp.')\n",
    "perm_imp = imp\n",
    "perm_imp_feats = X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial dependence plots\n",
    "for 5 features with highest permutation importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_imp_idx = np.argsort(perm_imp)[:-6:-1]\n",
    "features = X.columns[most_imp_idx]\n",
    "for feat in features:\n",
    "    ax = plot_pdp(clf, X_train[:30000], feat, \n",
    "             scaler=scaler, \n",
    "             column_names = X.columns,\n",
    "             query='',\n",
    "             show_deciles=True,\n",
    "             show_distplot=True,\n",
    "             y=y_train[:30000],\n",
    "             pardep_kws=dict(percentiles=(0.1,0.9)),\n",
    "            )\n",
    "    exp.log_figure(f\"pdp_{feat}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model explainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = '\\n#\\n'.join(In)\n",
    "exp.set_code(code=code)\n",
    "In.clear()\n",
    "exp.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "notify_time": "10",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
